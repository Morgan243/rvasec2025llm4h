
@misc{meng_locating_2023,
	title = {Locating and Editing Factual Associations in {GPT}},
	url = {http://arxiv.org/abs/2202.05262},
	doi = {10.48550/arXiv.2202.05262},
	abstract = {We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing ({ROME}). We find that {ROME} is effective on a standard zero-shot relation extraction ({zsRE}) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate {ROME} on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/},
	number = {{arXiv}:2202.05262},
	publisher = {{arXiv}},
	author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
	urldate = {2025-03-30},
	date = {2023-01-13},
	eprinttype = {arxiv},
	eprint = {2202.05262 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/morgan/Zotero/storage/JEDJEPEN/Meng et al. - 2023 - Locating and Editing Factual Associations in GPT.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/R7VB2NE6/2202.html:text/html},
}

@report{vassilev_adversarial_2024,
	location = {Gaithersburg, {MD}},
	title = {Adversarial machine learning : a taxonomy and terminology of attacks and mitigations},
	url = {https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.pdf},
	shorttitle = {Adversarial machine learning},
	abstract = {This {NIST} {AI} report develops a taxonomy of concepts and defines terminology in the field of adversarial machine learning ({AML}). The taxonomy is built on survey of the {AML} literature and is arranged in a conceptual hierarchy that includes key types of {ML} methods and lifecycle stage of attack, attacker goals and objectives, and attacker capabilities and knowledge of the learning process. The report also provides corresponding methods for mitigating and managing the consequences of attacks and points out relevant open challenges to take into account in the lifecycle of {AI} systems. The terminology used in the report is consistent with the literature on {AML} and is complemented by a glossary that defines key terms associated with the security of {AI} systems and is intended to assist non-expert readers. Taken together, the taxonomy and terminology are meant to inform other standards and future practice guides for assessing and managing the security of {AI} systems, by establishing a common language and understanding of the rapidly developing {AML} landscape.},
	pages = {NIST  100--2e2023},
	number = {{NIST}  100-2e2023},
	institution = {National Institute of Standards and Technology (U.S.)},
	author = {Vassilev, Apostol and Oprea, Alina and Fordyce, Alie and Anderson, Hyrum},
	urldate = {2025-03-28},
	date = {2024-01-04},
	langid = {english},
	doi = {10.6028/NIST.AI.100-2e2023},
	keywords = {rvasec25:main\_topic},
	file = {PDF:/home/morgan/Zotero/storage/XKPB4XSA/Vassilev et al. - 2024 - Adversarial machine learning  a taxonomy and terminology of attacks and mitigations.pdf:application/pdf},
}

@report{national_institute_of_standards_and_technology_us_artificial_2024,
	location = {Gaithersburg, {MD}},
	title = {Artificial intelligence risk management framework : generative artificial intelligence profile},
	url = {https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf},
	shorttitle = {Artificial intelligence risk management framework},
	abstract = {This document is a cross-sectoral profile of and companion resource for the {AI} Risk Management Framework ({AI} {RMF} 1.0) for Generative {AI}, 1 pursuant to President Biden’s Executive Order ({EO}) 14110 on Safe, Secure, and Trustworthy Artificial Intelligence.2 The {AI} {RMF} was released in January 2023, and is intended for voluntary use and to improve the ability of organizations to incorporate trustworthiness considerations into the design, development, use, and evaluation of {AI} products, services, and systems.},
	pages = {error:  600--1},
	number = {error:  600-1},
	institution = {National Institute of Standards and Technology (U.S.)},
	author = {{National Institute of Standards and Technology (US)}},
	urldate = {2025-03-28},
	date = {2024-07-26},
	langid = {english},
	doi = {10.6028/NIST.AI.600-1},
	keywords = {rvasec25:main\_topic},
	file = {PDF:/home/morgan/Zotero/storage/FPKRE9GB/National Institute of Standards and Technology (US) - 2024 - Artificial intelligence risk management framework  generative artificial intelligence profile.pdf:application/pdf},
}

@misc{subramanian_small_2025,
	title = {Small Language Models ({SLMs}) Can Still Pack a Punch: A survey},
	url = {http://arxiv.org/abs/2501.05465},
	doi = {10.48550/arXiv.2501.05465},
	shorttitle = {Small Language Models ({SLMs}) Can Still Pack a Punch},
	abstract = {As foundation {AI} models continue to increase in size, an important question arises - is massive scale the only path forward? This survey of about 160 papers presents a family of Small Language Models ({SLMs}) in the 1 to 8 billion parameter range that demonstrate smaller models can perform as well, or even outperform large models. We explore task agnostic, general purpose {SLMs}, task-specific {SLMs} and techniques to create {SLMs} that can guide the community to build models while balancing performance, efficiency, scalability and cost. Furthermore we define and characterize {SLMs}' effective sizes, representing increased capability with respect to {LLMs}.},
	number = {{arXiv}:2501.05465},
	publisher = {{arXiv}},
	author = {Subramanian, Shreyas and Elango, Vikram and Gungor, Mecit},
	urldate = {2025-04-05},
	date = {2025-01-03},
	eprinttype = {arxiv},
	eprint = {2501.05465 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/morgan/Zotero/storage/SBTU5YLV/Subramanian et al. - 2025 - Small Language Models (SLMs) Can Still Pack a Punch A survey.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/NTLGU4W4/2501.html:text/html},
}

@inproceedings{musaeus_iterative_2025,
	location = {Abu Dhabi, {UAE}},
	title = {Iterative Structured Knowledge Distillation: Optimizing Language Models Through Layer-by-Layer Distillation},
	url = {https://aclanthology.org/2025.coling-main.440/},
	shorttitle = {Iterative Structured Knowledge Distillation},
	abstract = {Traditional language model compression techniques, like knowledge distillation, require a fixed architecture, limiting flexibility, while structured pruning methods often fail to preserve performance. This paper introduces Iterative Structured Knowledge Distillation ({ISKD}), which integrates knowledge distillation and structured pruning by progressively replacing transformer blocks with smaller, efficient versions during training. This study validates {ISKD} on two transformer-based language models: {GPT}-2 and Phi-1. {ISKD} outperforms L1 pruning and achieves similar performance to knowledge distillation while offering greater flexibility. {ISKD} reduces model parameters - 30.68\% for {GPT}-2 and 30.16\% for Phi-1 - while maintaining at least four-fifths of performance on both language modeling and commonsense reasoning tasks. These findings suggest that this method offers a promising balance between model efficiency and accuracy.},
	eventtitle = {{COLING} 2025},
	pages = {6601--6606},
	booktitle = {Proceedings of the 31st International Conference on Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Musaeus, Malthe Have and van der Goot, Rob},
	editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven},
	urldate = {2025-04-05},
	date = {2025-01},
	file = {Full Text PDF:/home/morgan/Zotero/storage/PG6F5NWL/Musaeus and van der Goot - 2025 - Iterative Structured Knowledge Distillation Optimizing Language Models Through Layer-by-Layer Disti.pdf:application/pdf},
}

@misc{razzhigaev_llm-microscope_2025,
	title = {{LLM}-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers},
	url = {http://arxiv.org/abs/2502.15007},
	doi = {10.48550/arXiv.2502.15007},
	shorttitle = {{LLM}-Microscope},
	abstract = {We introduce methods to quantify how Large Language Models ({LLMs}) encode and store contextual information, revealing that tokens often seen as minor (e.g., determiners, punctuation) carry surprisingly high context. Notably, removing these tokens -- especially stopwords, articles, and commas -- consistently degrades performance on {MMLU} and {BABILong}-4k, even if removing only irrelevant tokens. Our analysis also shows a strong correlation between contextualization and linearity, where linearity measures how closely the transformation from one layer's embeddings to the next can be approximated by a single linear mapping. These findings underscore the hidden importance of filler tokens in maintaining context. For further exploration, we present {LLM}-Microscope, an open-source toolkit that assesses token-level nonlinearity, evaluates contextual memory, visualizes intermediate layer contributions (via an adapted Logit Lens), and measures the intrinsic dimensionality of representations. This toolkit illuminates how seemingly trivial tokens can be critical for long-range understanding.},
	number = {{arXiv}:2502.15007},
	publisher = {{arXiv}},
	author = {Razzhigaev, Anton and Mikhalchuk, Matvey and Rahmatullaev, Temurbek and Goncharova, Elizaveta and Druzhinina, Polina and Oseledets, Ivan and Kuznetsov, Andrey},
	urldate = {2025-04-05},
	date = {2025-02-20},
	eprinttype = {arxiv},
	eprint = {2502.15007 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/morgan/Zotero/storage/IMR9XN26/Razzhigaev et al. - 2025 - LLM-Microscope Uncovering the Hidden Role of Punctuation in Context Memory of Transformers.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/WT7ZBQZ3/2502.html:text/html},
}

@misc{quirke_tinysql_2025,
	title = {{TinySQL}: A Progressive Text-to-{SQL} Dataset for Mechanistic Interpretability Research},
	url = {http://arxiv.org/abs/2503.12730},
	doi = {10.48550/arXiv.2503.12730},
	shorttitle = {{TinySQL}},
	abstract = {Mechanistic interpretability research faces a gap between analyzing simple circuits in toy tasks and discovering features in large models. To bridge this gap, we propose text-to-{SQL} generation as an ideal task to study, as it combines the formal structure of toy tasks with real-world complexity. We introduce {TinySQL}, a synthetic dataset progressing from basic to advanced {SQL} operations, and train models ranging from 33M to 1B parameters to establish a comprehensive testbed for interpretability. We apply multiple complementary interpretability techniques, including edge attribution patching and sparse autoencoders, to identify minimal circuits and components supporting {SQL} generation. Our analysis reveals both the potential and limitations of current interpretability methods, showing how circuits can vary even across similar queries. Lastly, we demonstrate how mechanistic interpretability can identify flawed heuristics in models and improve synthetic dataset design. Our work provides a comprehensive framework for evaluating and advancing interpretability techniques while establishing clear boundaries for their reliable application.},
	number = {{arXiv}:2503.12730},
	publisher = {{arXiv}},
	author = {Quirke, Philip and Neo, Clement and Harrasse, Abir and Nathawani, Dhruv and Abdullah, Amir},
	urldate = {2025-04-05},
	date = {2025-03-17},
	eprinttype = {arxiv},
	eprint = {2503.12730 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Databases},
	file = {Preprint PDF:/home/morgan/Zotero/storage/F247EZGM/Quirke et al. - 2025 - TinySQL A Progressive Text-to-SQL Dataset for Mechanistic Interpretability Research.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/DR4J2XME/2503.html:text/html},
}

@misc{lai_mediator_2025,
	title = {Mediator: Memory-efficient {LLM} Merging with Less Parameter Conflicts and Uncertainty Based Routing},
	url = {http://arxiv.org/abs/2502.04411},
	doi = {10.48550/arXiv.2502.04411},
	shorttitle = {Mediator},
	abstract = {Model merging aggregates Large Language Models ({LLMs}) finetuned on different tasks into a stronger one. However, parameter conflicts between models leads to performance degradation in averaging. While model routing addresses this issue by selecting individual models during inference, it imposes excessive storage and compute costs, and fails to leverage the common knowledge from different models. In this work, we observe that different layers exhibit varying levels of parameter conflicts. Building on this insight, we average layers with minimal parameter conflicts and use a novel task-level expert routing for layers with significant conflicts. To further reduce storage costs, inspired by task arithmetic sparsity, we decouple multiple fine-tuned experts into a dense expert and several sparse experts. Considering the out-of-distribution samples, we select and merge appropriate experts based on the task uncertainty of the input data. We conduct extensive experiments on both {LLaMA} and Qwen with varying parameter scales, and evaluate on real-world reasoning tasks. Results demonstrate that our method consistently achieves significant performance improvements while requiring less system cost compared to existing methods.},
	number = {{arXiv}:2502.04411},
	publisher = {{arXiv}},
	author = {Lai, Kunfeng and Tang, Zhenheng and Pan, Xinglin and Dong, Peijie and Liu, Xiang and Chen, Haolan and Shen, Li and Li, Bo and Chu, Xiaowen},
	urldate = {2025-04-05},
	date = {2025-02-11},
	eprinttype = {arxiv},
	eprint = {2502.04411 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/morgan/Zotero/storage/22XG45V5/Lai et al. - 2025 - Mediator Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/SM9XD382/2502.html:text/html},
}

@article{michaud_quantization_2023,
	title = {The Quantization Model of Neural Scaling},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/5b6346a05a537d4cdb2f50323452a9fe-Abstract-Conference.html},
	pages = {28699--28722},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Michaud, Eric and Liu, Ziming and Girit, Uzay and Tegmark, Max},
	urldate = {2025-04-05},
	date = {2023-12-15},
	langid = {english},
	file = {Full Text PDF:/home/morgan/Zotero/storage/Z4ZJ2WPZ/Michaud et al. - 2023 - The Quantization Model of Neural Scaling.pdf:application/pdf},
}

@misc{allen-zhu_physics_2024,
	title = {Physics of Language Models: Part 3.1, Knowledge Storage and Extraction},
	url = {http://arxiv.org/abs/2309.14316},
	doi = {10.48550/arXiv.2309.14316},
	shorttitle = {Physics of Language Models},
	abstract = {Large language models ({LLMs}) can store a vast amount of world knowledge, often extractable via question-answering (e.g., "What is Abraham Lincoln's birthday?"). However, do they answer such questions based on exposure to similar questions during training (i.e., cheating), or by genuinely learning to extract knowledge from sources like Wikipedia? In this paper, we investigate this issue using a controlled biography dataset. We find a strong correlation between the model's ability to extract knowledge and various diversity measures of the training data. \${\textbackslash}textbf\{Essentially\}\$, for knowledge to be reliably extracted, it must be sufficiently augmented (e.g., through paraphrasing, sentence shuffling, translations) \${\textbackslash}textit\{during pretraining\}\$. Without such augmentation, knowledge may be memorized but not extractable, leading to 0\% accuracy, regardless of subsequent instruction fine-tuning. To understand why this occurs, we employ (nearly) linear probing to demonstrate a strong connection between the observed correlation and how the model internally encodes knowledge -- whether it is linearly encoded in the hidden embeddings of entity names or distributed across other token embeddings in the training text. This paper provides \${\textbackslash}textbf\{several key recommendations for {LLM} pretraining in the industry\}\$: (1) rewrite the pretraining data -- using small, auxiliary models -- to provide knowledge augmentation, and (2) incorporate more instruction-finetuning data into the pretraining stage before it becomes too late.},
	number = {{arXiv}:2309.14316},
	publisher = {{arXiv}},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
	urldate = {2025-04-05},
	date = {2024-07-16},
	eprinttype = {arxiv},
	eprint = {2309.14316 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/home/morgan/Zotero/storage/U77LQJGJ/Allen-Zhu and Li - 2024 - Physics of Language Models Part 3.1, Knowledge Storage and Extraction.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/GX6I3HD5/2309.html:text/html},
}

@misc{gromov_unreasonable_2025,
	title = {The Unreasonable Ineffectiveness of the Deeper Layers},
	url = {http://arxiv.org/abs/2403.17887},
	doi = {10.48550/arXiv.2403.17887},
	abstract = {How is knowledge stored in an {LLM}'s weights? We study this via layer pruning: if removing a certain layer does not affect model performance in common question-answering benchmarks, then the weights in that layer are not necessary for storing the knowledge needed to answer those questions. To find these unnecessary parameters, we identify the optimal block of layers to prune by considering similarity across layers; then, to "heal" the damage, we perform a small amount of finetuning. Surprisingly, with this method we find minimal degradation of performance until after a large fraction (up to half) of the layers are removed for some common open-weight models. From a scientific perspective, the robustness of these {LLMs} to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge. For our study, we use parameter-efficient finetuning ({PEFT}) methods, specifically quantization and Low Rank Adapters ({QLoRA}), such that each of our experiments can be performed on a single 40GB A100 {GPU}.},
	number = {{arXiv}:2403.17887},
	publisher = {{arXiv}},
	author = {Gromov, Andrey and Tirumala, Kushal and Shapourian, Hassan and Glorioso, Paolo and Roberts, Daniel A.},
	urldate = {2025-04-05},
	date = {2025-03-03},
	eprinttype = {arxiv},
	eprint = {2403.17887 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {Full Text PDF:/home/morgan/Zotero/storage/S3FZD8JT/Gromov et al. - 2025 - The Unreasonable Ineffectiveness of the Deeper Layers.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/TQI4BH4F/2403.html:text/html},
}

@article{maini_llm_2024,
	title = {{LLM} Dataset Inference: Did you train on my dataset?},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/e01519b47118e2f51aa643151350c905-Abstract-Conference.html},
	shorttitle = {{LLM} Dataset Inference},
	pages = {124069--124092},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Maini, Pratyush and Jia, Hengrui and Papernot, Nicolas and Dziedzic, Adam},
	urldate = {2025-04-05},
	date = {2024-12-16},
	langid = {english},
	file = {Full Text PDF:/home/morgan/Zotero/storage/UVEEGHD8/Maini et al. - 2024 - LLM Dataset Inference Did you train on my dataset.pdf:application/pdf},
}

@article{maini_llm_nodate,
	title = {{LLM} Dataset Inference: Did you train on my dataset?},
	abstract = {The proliferation of large language models ({LLMs}) in the real world has come with a rise in copyright cases against companies for training their models on unlicensed data from the internet. Recent works have presented methods to identify if individual text sequences were members of the model’s training data, known as membership inference attacks ({MIAs}). We demonstrate that the apparent success of these {MIAs} is confounded by selecting non-members (text sequences not used for training) belonging to a different distribution from the members (e.g., temporally shifted recent Wikipedia articles compared with ones used to train the model). This distribution shift makes membership inference appear successful. However, most {MIA} methods perform no better than random guessing when discriminating between members and non-members from the same distribution (e.g., in this case, the same period of time). Even when {MIAs} work, we find that different {MIAs} succeed at inferring membership of samples from different distributions. Instead, we propose a new dataset inference method to accurately identify the datasets used to train large language models. This paradigm sits realistically in the modern-day copyright landscape, where authors claim that an {LLM} is trained over multiple documents (such as a book) written by them, rather than one particular paragraph. While dataset inference shares many of the challenges of membership inference, we solve it by selectively combining the {MIAs} that provide positive signal for a given distribution, and aggregating them to perform a statistical test on a given dataset. Our approach successfully distinguishes the train and test sets of different subsets of the Pile with statistically significant p-values {\textless} 0.1, without any false positives.},
	author = {Maini, Pratyush and Jia, Hengrui and Papernot, Nicolas and Dziedzic, Adam},
	langid = {english},
	file = {PDF:/home/morgan/Zotero/storage/T3I4FJ2M/Maini et al. - LLM Dataset Inference Did you train on my dataset.pdf:application/pdf},
}

@online{noauthor_roneneldantinystories_2025,
	title = {roneneldan/{TinyStories} · Datasets at Hugging Face},
	url = {https://huggingface.co/datasets/roneneldan/TinyStories},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2025-04-05},
	date = {2025-03-16},
	file = {Snapshot:/home/morgan/Zotero/storage/RHKBPQSM/TinyStories.html:text/html},
}

@misc{eldan_tinystories_2023,
	title = {{TinyStories}: How Small Can Language Models Be and Still Speak Coherent English?},
	url = {http://arxiv.org/abs/2305.07759},
	doi = {10.48550/arXiv.2305.07759},
	shorttitle = {{TinyStories}},
	abstract = {Language models ({LMs}) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as {GPT}-Neo (small) or {GPT}-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce {TinyStories}, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by {GPT}-3.5 and {GPT}-4. We show that {TinyStories} can be used to train and evaluate {LMs} that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses {GPT}-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that {TinyStories} can facilitate the development, analysis and research of {LMs}, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in {LMs}.},
	number = {{arXiv}:2305.07759},
	publisher = {{arXiv}},
	author = {Eldan, Ronen and Li, Yuanzhi},
	urldate = {2025-04-05},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.07759 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/morgan/Zotero/storage/SIRLRLTP/Eldan and Li - 2023 - TinyStories How Small Can Language Models Be and Still Speak Coherent English.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/JCRRFN8Z/2305.html:text/html},
}

@misc{huang_thriftllm_2025,
	title = {{ThriftLLM}: On Cost-Effective Selection of Large Language Models for Classification Queries},
	url = {http://arxiv.org/abs/2501.04901},
	doi = {10.48550/arXiv.2501.04901},
	shorttitle = {{ThriftLLM}},
	abstract = {In recent years, large language models ({LLMs}) have demonstrated remarkable capabilities in comprehending and generating natural language content. An increasing number of services offer {LLMs} for various tasks via {APIs}. Different {LLMs} demonstrate expertise in different domains of queries (e.g., text classification queries). Meanwhile, {LLMs} of different scales, complexity, and performance are priced diversely. Driven by this, several researchers are investigating strategies for selecting an ensemble of {LLMs}, aiming to decrease overall usage costs while enhancing performance. However, to the best of our knowledge, none of the existing works addresses the problem, how to find an {LLM} ensemble subject to a cost budget, which maximizes the ensemble performance with guarantees. In this paper, we formalize the performance of an ensemble of models ({LLMs}) using the notion of prediction accuracy which we formally define. We develop an approach for aggregating responses from multiple {LLMs} to enhance ensemble performance. Building on this, we formulate the Optimal Ensemble Selection problem of selecting a set of {LLMs} subject to a cost budget that maximizes the overall prediction accuracy. We show that prediction accuracy is non-decreasing and non-submodular and provide evidence that the Optimal Ensemble Selection problem is likely to be {NP}-hard. By leveraging a submodular function that upper bounds prediction accuracy, we develop an algorithm called {ThriftLLM} and prove that it achieves an instance-dependent approximation guarantee with high probability. In addition, it achieves state-of-the-art performance for text classification and entity matching queries on multiple real-world datasets against various baselines in our extensive experimental evaluation, while using a relatively lower cost budget, strongly supporting the effectiveness and superiority of our method.},
	number = {{arXiv}:2501.04901},
	publisher = {{arXiv}},
	author = {Huang, Keke and Shi, Yimin and Ding, Dujian and Li, Yifei and Fei, Yang and Lakshmanan, Laks and Xiao, Xiaokui},
	urldate = {2025-04-05},
	date = {2025-02-02},
	eprinttype = {arxiv},
	eprint = {2501.04901 [cs]},
	keywords = {Computer Science - Databases},
	file = {Preprint PDF:/home/morgan/Zotero/storage/REG8UL9Y/Huang et al. - 2025 - ThriftLLM On Cost-Effective Selection of Large Language Models for Classification Queries.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/NGWBND3Y/2501.html:text/html},
}

@article{lamaakal_tiny_2025,
	title = {Tiny Language Models for Automation and Control: Overview, Potential Applications, and Future Research Directions},
	volume = {25},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/25/5/1318},
	doi = {10.3390/s25051318},
	shorttitle = {Tiny Language Models for Automation and Control},
	abstract = {Large Language Models ({LLMs}), like {GPT} and {BERT}, have significantly advanced Natural Language Processing ({NLP}), enabling high performance on complex tasks. However, their size and computational needs make {LLMs} unsuitable for deployment on resource-constrained devices, where efficiency, speed, and low power consumption are critical. Tiny Language Models ({TLMs}), also known as {BabyLMs}, offer compact alternatives by using advanced compression and optimization techniques to function effectively on devices such as smartphones, Internet of Things ({IoT}) systems, and embedded platforms. This paper provides a comprehensive survey of {TLM} architectures and methodologies, including key techniques such as knowledge distillation, quantization, and pruning. Additionally, it explores potential and emerging applications of {TLMs} in automation and control, covering areas such as edge computing, {IoT}, industrial automation, and healthcare. The survey discusses challenges unique to {TLMs}, such as trade-offs between model size and accuracy, limited generalization, and ethical considerations in deployment. Future research directions are also proposed, focusing on hybrid compression techniques, application-specific adaptations, and context-aware {TLMs} optimized for hardware-specific constraints. This paper aims to serve as a foundational resource for advancing {TLMs} capabilities across diverse real-world applications.},
	pages = {1318},
	number = {5},
	journaltitle = {Sensors},
	author = {Lamaakal, Ismail and Maleh, Yassine and El Makkaoui, Khalid and Ouahbi, Ibrahim and Pławiak, Paweł and Alfarraj, Osama and Almousa, May and Abd El-Latif, Ahmed A.},
	urldate = {2025-04-05},
	date = {2025-01},
	langid = {english},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {optimization, {IoT}, edge computing, large language models, model compression, small language models, tiny language models},
	file = {Full Text PDF:/home/morgan/Zotero/storage/HD5LU7MQ/Lamaakal et al. - 2025 - Tiny Language Models for Automation and Control Overview, Potential Applications, and Future Resear.pdf:application/pdf},
}

@misc{hillier_super_2024,
	title = {Super Tiny Language Models},
	url = {http://arxiv.org/abs/2405.14159},
	doi = {10.48550/arXiv.2405.14159},
	abstract = {The rapid advancement of large language models ({LLMs}) has led to significant improvements in natural language processing but also poses challenges due to their high computational and energy demands. This paper introduces a series of research efforts focused on Super Tiny Language Models ({STLMs}), which aim to deliver high performance with significantly reduced parameter counts. We explore innovative techniques such as byte-level tokenization with a pooling mechanism, weight tying, and efficient training strategies. These methods aim to significantly reduce reduce the parameter count compared to traditional models -- in future works, we aim to build on these in a way that maintains and improves upon the performance of base transformer models. This series of papers will explore into various subproblems, including tokenizer-free models, self-play based training, and alternative training objectives. We will target models with 10M, 50M, and 100M parameters. Our ultimate goal is to make high-performance language models more accessible and practical for a wide range of applications.},
	number = {{arXiv}:2405.14159},
	publisher = {{arXiv}},
	author = {Hillier, Dylan and Guertler, Leon and Tan, Cheston and Agrawal, Palaash and Ruirui, Chen and Cheng, Bobby},
	urldate = {2025-04-05},
	date = {2024-06-26},
	eprinttype = {arxiv},
	eprint = {2405.14159 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/morgan/Zotero/storage/RPHJZP7B/Hillier et al. - 2024 - Super Tiny Language Models.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/SXP5IRKI/2405.html:text/html},
}

@misc{hu_minicpm_2024,
	title = {{MiniCPM}: Unveiling the Potential of Small Language Models with Scalable Training Strategies},
	url = {http://arxiv.org/abs/2404.06395},
	doi = {10.48550/arXiv.2404.06395},
	shorttitle = {{MiniCPM}},
	abstract = {The burgeoning interest in developing Large Language Models ({LLMs}) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models ({SLMs}) as a resource-efficient alternative. In this context, we introduce {MiniCPM}, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B {LLMs}. While focusing on {SLMs}, our approach exhibits scalability in both model and data dimensions for future {LLM} research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay ({WSD}) learning rate scheduler ({LRS}), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the {WSD} {LRS}. With {WSD} {LRS}, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce {MiniCPM} family, including {MiniCPM}-{DPO}, {MiniCPM}-{MoE} and {MiniCPM}-128K, whose excellent performance further cementing {MiniCPM}'s foundation in diverse {SLM} applications. {MiniCPM} models are available publicly at https://github.com/{OpenBMB}/{MiniCPM} .},
	number = {{arXiv}:2404.06395},
	publisher = {{arXiv}},
	author = {Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and Zhang, Xinrong and Thai, Zheng Leng and Zhang, Kaihuo and Wang, Chongyi and Yao, Yuan and Zhao, Chenyang and Zhou, Jie and Cai, Jie and Zhai, Zhongwu and Ding, Ning and Jia, Chao and Zeng, Guoyang and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
	urldate = {2025-04-04},
	date = {2024-06-03},
	eprinttype = {arxiv},
	eprint = {2404.06395 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Full Text PDF:/home/morgan/Zotero/storage/L4U5W8ZF/Hu et al. - 2024 - MiniCPM Unveiling the Potential of Small Language Models with Scalable Training Strategies.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/6S3RXFNG/2404.html:text/html},
}

@misc{laurencon_building_2024,
	title = {Building and better understanding vision-language models: insights and future directions},
	url = {http://arxiv.org/abs/2408.12637},
	doi = {10.48550/arXiv.2408.12637},
	shorttitle = {Building and better understanding vision-language models},
	abstract = {The field of vision-language models ({VLMs}), which take images and texts as inputs and output texts, is rapidly evolving and has yet to reach consensus on several key aspects of the development pipeline, including data, architecture, and training methods. This paper can be seen as a tutorial for building a {VLM}. We begin by providing a comprehensive overview of the current state-of-the-art approaches, highlighting the strengths and weaknesses of each, addressing the major challenges in the field, and suggesting promising research directions for underexplored areas. We then walk through the practical steps to build Idefics3-8B, a powerful {VLM} that significantly outperforms its predecessor Idefics2-8B, while being trained efficiently, exclusively on open datasets, and using a straightforward pipeline. These steps include the creation of Docmatix, a dataset for improving document understanding capabilities, which is 240 times larger than previously available datasets. We release the model along with the datasets created for its training.},
	number = {{arXiv}:2408.12637},
	publisher = {{arXiv}},
	author = {Laurençon, Hugo and Marafioti, Andrés and Sanh, Victor and Tronchon, Léo},
	urldate = {2025-04-04},
	date = {2024-08-22},
	eprinttype = {arxiv},
	eprint = {2408.12637 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/morgan/Zotero/storage/QWKUQUAB/Laurençon et al. - 2024 - Building and better understanding vision-language models insights and future directions.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/EMZDMQZN/2408.html:text/html},
}

@misc{hershcovitch_zipnn_2024,
	title = {{ZipNN}: Lossless Compression for {AI} Models},
	url = {http://arxiv.org/abs/2411.05239},
	doi = {10.48550/arXiv.2411.05239},
	shorttitle = {{ZipNN}},
	abstract = {With the growth of model sizes and the scale of their deployment, their sheer size burdens the infrastructure requiring more network and more storage to accommodate these. While there is a vast model compression literature deleting parts of the model weights for faster inference, we investigate a more traditional type of compression - one that represents the model in a compact form and is coupled with a decompression algorithm that returns it to its original form and size - namely lossless compression. We present {ZipNN} a lossless compression tailored to neural networks. Somewhat surprisingly, we show that specific lossless compression can gain significant network and storage reduction on popular models, often saving 33\% and at times reducing over 50\% of the model size. We investigate the source of model compressibility and introduce specialized compression variants tailored for models that further increase the effectiveness of compression. On popular models (e.g. Llama 3) {ZipNN} shows space savings that are over 17\% better than vanilla compression while also improving compression and decompression speeds by 62\%. We estimate that these methods could save over an {ExaByte} per month of network traffic downloaded from a large model hub like Hugging Face.},
	number = {{arXiv}:2411.05239},
	publisher = {{arXiv}},
	author = {Hershcovitch, Moshik and Wood, Andrew and Choshen, Leshem and Girmonsky, Guy and Leibovitz, Roy and Ennmouri, Ilias and Malka, Michal and Chin, Peter and Sundararaman, Swaminathan and Harnik, Danny},
	urldate = {2025-04-04},
	date = {2024-11-07},
	eprinttype = {arxiv},
	eprint = {2411.05239 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Theory, Mathematics - Information Theory},
	file = {Full Text PDF:/home/morgan/Zotero/storage/HL66ZICA/Hershcovitch et al. - 2024 - ZipNN Lossless Compression for AI Models.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/FHEWAD5K/2411.html:text/html},
}

@inproceedings{poli_hyena_2023,
	title = {Hyena Hierarchy: Towards Larger Convolutional Language Models},
	url = {https://proceedings.mlr.press/v202/poli23a.html},
	shorttitle = {Hyena Hierarchy},
	abstract = {Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers at scale, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In challenging reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-space models, transfer functions, and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets {WikiText}103 and The Pile, reaching Transformer quality with a 20\% reduction in training compute required at sequence length 2k. Hyena operators are 2x faster than highly optimized attention at sequence length 8k, with speedups of 100x at 64k.},
	eventtitle = {International Conference on Machine Learning},
	pages = {28043--28078},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y. and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and Re, Christopher},
	urldate = {2025-04-05},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/home/morgan/Zotero/storage/2R7JS6KH/Poli et al. - 2023 - Hyena Hierarchy Towards Larger Convolutional Language Models.pdf:application/pdf},
}

@article{das_security_2025,
	title = {Security and Privacy Challenges of Large Language Models: A Survey},
	volume = {57},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3712001},
	doi = {10.1145/3712001},
	shorttitle = {Security and Privacy Challenges of Large Language Models},
	abstract = {Large language models ({LLMs}) have demonstrated extraordinary capabilities and contributed to multiple fields, such as generating and summarizing text, language translation, and question-answering. Today, {LLMs} have become quite popular tools in natural language processing tasks, with the capability to analyze complicated linguistic patterns and provide relevant responses depending on the context. While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and personally identifiable information leakage attacks. This survey provides a thorough review of the security and privacy challenges of {LLMs}, along with the application-based risks in various domains, such as transportation, education, and healthcare. We assess the extent of {LLM} vulnerabilities, investigate emerging security and privacy attacks against {LLMs}, and review potential defense mechanisms. Additionally, the survey outlines existing research gaps and highlights future research directions.},
	pages = {1--39},
	number = {6},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Das, Badhan Chandra and Amini, M. Hadi and Wu, Yanzhao},
	urldate = {2025-04-05},
	date = {2025-06-30},
	langid = {english},
	file = {Full Text PDF:/home/morgan/Zotero/storage/J9PZNJRV/Das et al. - 2025 - Security and Privacy Challenges of Large Language Models A Survey.pdf:application/pdf},
}

@article{wang_mamba_2024,
	title = {The Mamba in the Llama: Distilling and Accelerating Hybrid Models},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/723933067ad315269b620bc0d2c05cba-Abstract-Conference.html},
	shorttitle = {The Mamba in the Llama},
	pages = {62432--62457},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Wang, Junxiong and Paliotta, Daniele and May, Avner and Rush, Alexander M. and Dao, Tri},
	urldate = {2025-04-05},
	date = {2024-12-16},
	langid = {english},
	file = {Full Text PDF:/home/morgan/Zotero/storage/VW85T29U/Wang et al. - 2024 - The Mamba in the Llama Distilling and Accelerating Hybrid Models.pdf:application/pdf},
}

@misc{dao_transformers_2024,
	title = {Transformers are {SSMs}: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
	url = {http://arxiv.org/abs/2405.21060},
	doi = {10.48550/arXiv.2405.21060},
	shorttitle = {Transformers are {SSMs}},
	abstract = {While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models ({SSMs}) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between {SSMs} and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality ({SSD}) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective {SSM} that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.},
	number = {{arXiv}:2405.21060},
	publisher = {{arXiv}},
	author = {Dao, Tri and Gu, Albert},
	urldate = {2025-04-05},
	date = {2024-05-31},
	eprinttype = {arxiv},
	eprint = {2405.21060 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/home/morgan/Zotero/storage/V7HUKEYS/Dao and Gu - 2024 - Transformers are SSMs Generalized Models and Efficient Algorithms Through Structured State Space Du.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/I8M8RQ3A/2405.html:text/html},
}

@misc{xu_survey_2024,
	title = {A Survey of Resource-efficient {LLM} and Multimodal Foundation Models},
	url = {http://arxiv.org/abs/2401.08092},
	doi = {10.48550/arXiv.2401.08092},
	abstract = {Large foundation models, including large language models ({LLMs}), vision transformers ({ViTs}), diffusion, and {LLM}-based multimodal models, are revolutionizing the entire machine learning lifecycle, from training to deployment. However, the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources. To support the growth of these large models in a scalable and environmentally sustainable way, there has been a considerable focus on developing resource-efficient strategies. This survey delves into the critical importance of such research, examining both algorithmic and systemic aspects. It offers a comprehensive analysis and valuable insights gleaned from existing literature, encompassing a broad array of topics from cutting-edge model architectures and training/serving algorithms to practical system designs and implementations. The goal of this survey is to provide an overarching understanding of how current approaches are tackling the resource challenges posed by large foundation models and to potentially inspire future breakthroughs in this field.},
	number = {{arXiv}:2401.08092},
	publisher = {{arXiv}},
	author = {Xu, Mengwei and Yin, Wangsong and Cai, Dongqi and Yi, Rongjie and Xu, Daliang and Wang, Qipeng and Wu, Bingyang and Zhao, Yihao and Yang, Chen and Wang, Shihe and Zhang, Qiyang and Lu, Zhenyan and Zhang, Li and Wang, Shangguang and Li, Yuanchun and Liu, Yunxin and Jin, Xin and Liu, Xuanzhe},
	urldate = {2025-04-05},
	date = {2024-09-23},
	eprinttype = {arxiv},
	eprint = {2401.08092 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/home/morgan/Zotero/storage/FBUBHNHX/Xu et al. - 2024 - A Survey of Resource-efficient LLM and Multimodal Foundation Models.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/K9D446P3/2401.html:text/html},
}

@article{ma_megalodon_2024,
	title = {Megalodon: Efficient {LLM} Pretraining and Inference with Unlimited Context Length},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/840abfadd04c967feaa2a49aba94a32d-Abstract-Conference.html},
	shorttitle = {Megalodon},
	pages = {71831--71854},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Ma, Xuezhe and Yang, Xiaomeng and Xiong, Wenhan and Chen, Beidi and Yu, Lili and Zhang, Hao and May, Jonathan and Zettlemoyer, Luke and Levy, Omer and Zhou, Chunting},
	urldate = {2025-04-05},
	date = {2024-12-16},
	langid = {english},
	file = {Full Text PDF:/home/morgan/Zotero/storage/34ECS6PN/Ma et al. - 2024 - Megalodon Efficient LLM Pretraining and Inference with Unlimited Context Length.pdf:application/pdf},
}

@misc{zhou_survey_2024,
	title = {A Survey on Efficient Inference for Large Language Models},
	url = {http://arxiv.org/abs/2404.14294},
	doi = {10.48550/arXiv.2404.14294},
	abstract = {Large Language Models ({LLMs}) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of {LLM} inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of {LLM} inference. This paper presents a comprehensive survey of the existing literature on efficient {LLM} inference. We start by analyzing the primary causes of the inefficient {LLM} inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.},
	number = {{arXiv}:2404.14294},
	publisher = {{arXiv}},
	author = {Zhou, Zixuan and Ning, Xuefei and Hong, Ke and Fu, Tianyu and Xu, Jiaming and Li, Shiyao and Lou, Yuming and Wang, Luning and Yuan, Zhihang and Li, Xiuhong and Yan, Shengen and Dai, Guohao and Zhang, Xiao-Ping and Dong, Yuhan and Wang, Yu},
	urldate = {2025-04-05},
	date = {2024-07-19},
	eprinttype = {arxiv},
	eprint = {2404.14294 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/morgan/Zotero/storage/MR3ZJPCC/Zhou et al. - 2024 - A Survey on Efficient Inference for Large Language Models.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/Y9DL5RTX/2404.html:text/html},
}

@online{noauthor_github_nodate,
	title = {{GitHub} - karpathy/{nanoGPT}: The simplest, fastest repository for training/finetuning medium-sized {GPTs}.},
	url = {https://github.com/karpathy/nanoGPT},
	urldate = {2025-04-05},
	file = {GitHub - karpathy/nanoGPT\: The simplest, fastest repository for training/finetuning medium-sized GPTs.:/home/morgan/Zotero/storage/SG65KQHL/nanoGPT.html:text/html},
}

@inproceedings{lin_malla_2024,
	title = {Malla: Demystifying Real-world Large Language Model Integrated Malicious Services},
	isbn = {978-1-939133-44-1},
	url = {https://www.usenix.org/conference/usenixsecurity24/presentation/lin-zilong},
	shorttitle = {Malla},
	eventtitle = {33rd {USENIX} Security Symposium ({USENIX} Security 24)},
	pages = {4693--4710},
	author = {Lin, Zilong and Cui, Jian and Liao, Xiaojing and Wang, {XiaoFeng}},
	urldate = {2025-04-05},
	date = {2024},
	langid = {english},
	file = {Full Text PDF:/home/morgan/Zotero/storage/LPSFT4DY/Lin et al. - 2024 - Malla Demystifying Real-world Large Language Model Integrated Malicious Services.pdf:application/pdf},
}

@article{malfa_language-models-as--service_2024,
	title = {Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges},
	volume = {80},
	rights = {Copyright (c) 2024 Journal of Artificial Intelligence Research},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/15865},
	doi = {10.1613/jair.1.15865},
	shorttitle = {Language-Models-as-a-Service},
	abstract = {Some of the most powerful language models currently are proprietary systems, accessible only via (typically restrictive) web or software programming interfaces. This is the Language-Models-as-a-Service ({LMaaS}) paradigm. In contrast with scenarios where full model access is available, as in the case of open-source models, such closed-off language models present specific challenges for evaluating, benchmarking, and testing them. This paper has two goals: on the one hand, we delineate how the aforementioned challenges act as impediments to the accessibility, reproducibility, reliability, and trustworthiness of {LMaaS}. We systematically examine the issues that arise from a lack of information about language models for each of these four aspects. We conduct a detailed analysis of existing solutions, put forth a number of recommendations, and highlight directions for future advancements. On the other hand, it serves as a synthesized overview of the licences and capabilities of the most popular {LMaaS}.},
	pages = {1497--1523},
	journaltitle = {Journal of Artificial Intelligence Research},
	author = {Malfa, Emanuele La and Petrov, Aleksandar and Frieder, Simon and Weinhuber, Christoph and Burnell, Ryan and Nazar, Raza and Cohn, Anthony and Shadbolt, Nigel and Wooldridge, Michael},
	urldate = {2025-04-05},
	date = {2024-08-26},
	langid = {english},
	keywords = {survey, Artificial Intelliegnce, language models, natural language},
	file = {Full Text PDF:/home/morgan/Zotero/storage/Q3RIIJFV/Malfa et al. - 2024 - Language-Models-as-a-Service Overview of a New Paradigm and its Challenges.pdf:application/pdf},
}

@article{lee_studying_2024,
	title = {Studying the Effects of Training Data on Small Language Models},
	url = {https://openreview.net/forum?id=4xBew7kuYB},
	abstract = {Prior work has found that training very small language models ({SLMs}) on synthetic children's stories allows them to generate coherent text, comparable to much larger models. These stories are claimed to encompass the vocabulary and factual knowledge base of a 3-4-year-old child, capturing the ``essence of natural language." Because of these claims, it is tempting to attribute the findings to the high readability (i.e., simple language) of children's stories, drawing a parallel to how children learn language. Is the human concept of readability relevant in the context of language model training, or are these findings better explained by other properties of the data? In this study, we investigate this by first validating several automatic readability measures. We then create synthetic corpora with varying levels of readability and assess the coherence of text generated by {SLMs} trained on these corpora. We find that training on high readability text is not a prerequisite for coherent {SLMs}. Specifically, {SLMs} trained on data with substantially more complex language also exhibit the same abilities as those trained on simple language. Moreover, training on simple language does not lead to the earlier development of coherence during training.},
	author = {Lee, Ivan and Berg-Kirkpatrick, Taylor},
	urldate = {2025-04-05},
	date = {2024-10-04},
	langid = {english},
	file = {Full Text PDF:/home/morgan/Zotero/storage/GPJZ3638/Lee and Berg-Kirkpatrick - 2024 - Studying the Effects of Training Data on Small Language Models.pdf:application/pdf},
}

@misc{dohmatob_strong_2024,
	title = {Strong Model Collapse},
	url = {http://arxiv.org/abs/2410.04840},
	doi = {10.48550/arXiv.2410.04840},
	abstract = {Within the scaling laws paradigm, which underpins the training of large neural networks like {ChatGPT} and Llama, we consider a supervised regression setting and establish the existance of a strong form of the model collapse phenomenon, a critical performance degradation due to synthetic data in the training corpus. Our results show that even the smallest fraction of synthetic data (e.g., as little as 1{\textbackslash}\% of the total training dataset) can still lead to model collapse: larger and larger training sets do not enhance performance. We further investigate whether increasing model size, an approach aligned with current trends in training large language models, exacerbates or mitigates model collapse. In a simplified regime where neural networks are approximated via random projections of tunable size, we both theoretically and empirically show that larger models can amplify model collapse. Interestingly, our theory also indicates that, beyond the interpolation threshold (which can be extremely high for very large datasets), larger models may mitigate the collapse, although they do not entirely prevent it. Our theoretical findings are empirically verified through experiments on language models and feed-forward neural networks for images.},
	number = {{arXiv}:2410.04840},
	publisher = {{arXiv}},
	author = {Dohmatob, Elvis and Feng, Yunzhen and Subramonian, Arjun and Kempe, Julia},
	urldate = {2025-04-05},
	date = {2024-10-08},
	eprinttype = {arxiv},
	eprint = {2410.04840 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/morgan/Zotero/storage/5T3KVRHQ/Dohmatob et al. - 2024 - Strong Model Collapse.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/IIANF43J/2410.html:text/html},
}

@misc{wu_prompt_2024,
	title = {Prompt Public Large Language Models to Synthesize Data for Private On-device Applications},
	url = {http://arxiv.org/abs/2404.04360},
	doi = {10.48550/arXiv.2404.04360},
	abstract = {Pre-training on public data is an effective method to improve the performance for federated learning ({FL}) with differential privacy ({DP}). This paper investigates how large language models ({LLMs}) trained on public data can improve the quality of pre-training data for the on-device language models trained with {DP} and {FL}. We carefully design {LLM} prompts to filter and transform existing public data, and generate new data to resemble the real user data distribution. The model pre-trained on our synthetic dataset achieves relative improvement of 19.0\% and 22.8\% in next word prediction accuracy compared to the baseline model pre-trained on a standard public dataset, when evaluated over the real user data in Gboard (Google Keyboard, a production mobile keyboard application). Furthermore, our method achieves evaluation accuracy better than or comparable to the baseline during the {DP} {FL} fine-tuning over millions of mobile devices, and our final model outperforms the baseline in production A/B testing. Our experiments demonstrate the strengths of {LLMs} in synthesizing data close to the private distribution even without accessing the private data, and also suggest future research directions to further reduce the distribution gap.},
	number = {{arXiv}:2404.04360},
	publisher = {{arXiv}},
	author = {Wu, Shanshan and Xu, Zheng and Zhang, Yanxiang and Zhang, Yuanbo and Ramage, Daniel},
	urldate = {2025-04-05},
	date = {2024-08-07},
	eprinttype = {arxiv},
	eprint = {2404.04360 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/home/morgan/Zotero/storage/BIB4K2UX/Wu et al. - 2024 - Prompt Public Large Language Models to Synthesize Data for Private On-device Applications.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/N533ZVQW/2404.html:text/html},
}

@misc{li_reason_2024,
	title = {Reason from Fallacy: Enhancing Large Language Models' Logical Reasoning through Logical Fallacy Understanding},
	url = {http://arxiv.org/abs/2404.04293},
	doi = {10.48550/arXiv.2404.04293},
	shorttitle = {Reason from Fallacy},
	abstract = {Large Language Models ({LLMs}) have demonstrated good performance in many reasoning tasks, but they still struggle with some complicated reasoning tasks including logical reasoning. One non-negligible reason for {LLMs}' suboptimal performance on logical reasoning is their overlooking of understanding logical fallacies correctly. To evaluate {LLMs}' capability of logical fallacy understanding ({LFU}), we propose five concrete tasks from three cognitive dimensions of {WHAT}, {WHY}, and {HOW} in this paper. Towards these {LFU} tasks, we have successfully constructed a new dataset {LFUD} based on {GPT}-4 accompanied by a little human effort. Our extensive experiments justify that our {LFUD} can be used not only to evaluate {LLMs}' {LFU} capability, but also to fine-tune {LLMs} to obtain significantly enhanced performance on logical reasoning.},
	number = {{arXiv}:2404.04293},
	publisher = {{arXiv}},
	author = {Li, Yanda and Wang, Dixuan and Liang, Jiaqing and Jiang, Guochao and He, Qianyu and Xiao, Yanghua and Yang, Deqing},
	urldate = {2025-04-05},
	date = {2024-04-04},
	eprinttype = {arxiv},
	eprint = {2404.04293 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/morgan/Zotero/storage/5BUCQ7TJ/Li et al. - 2024 - Reason from Fallacy Enhancing Large Language Models' Logical Reasoning through Logical Fallacy Unde.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/47564IT3/2404.html:text/html},
}

@article{yao_survey_2024,
	title = {A survey on large language model ({LLM}) security and privacy: The Good, The Bad, and The Ugly},
	volume = {4},
	issn = {2667-2952},
	url = {https://www.sciencedirect.com/science/article/pii/S266729522400014X},
	doi = {10.1016/j.hcc.2024.100211},
	shorttitle = {A survey on large language model ({LLM}) security and privacy},
	abstract = {Large Language Models ({LLMs}), such as {ChatGPT} and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, {LLMs} have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of {LLMs} with security and privacy. Specifically, we investigate how {LLMs} positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within {LLMs}. Through a comprehensive literature review, the paper categorizes the papers into “The Good” (beneficial {LLM} applications), “The Bad” (offensive applications), and “The Ugly” (vulnerabilities of {LLMs} and their defenses). We have some interesting findings. For example, {LLMs} have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by {LLM} parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the {LLMs}’ potential to both bolster and jeopardize cybersecurity.},
	pages = {100211},
	number = {2},
	journaltitle = {High-Confidence Computing},
	shortjournal = {High-Confidence Computing},
	author = {Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
	urldate = {2025-04-07},
	date = {2024-06-01},
	keywords = {{ChatGPT}, Large Language Model ({LLM}), {LLM} attacks, {LLM} privacy, {LLM} security, {LLM} vulnerabilities},
	file = {PDF:/home/morgan/Zotero/storage/ZRHHHEMS/Yao et al. - 2024 - A survey on large language model (LLM) security and privacy The Good, The Bad, and The Ugly.pdf:application/pdf;ScienceDirect Snapshot:/home/morgan/Zotero/storage/98PPFMTP/S266729522400014X.html:text/html;Submitted Version:/home/morgan/Zotero/storage/JLQUJTGX/Yao et al. - 2024 - A survey on large language model (LLM) security and privacy The Good, The Bad, and The Ugly.pdf:application/pdf},
}

@online{noauthor_dr_2013,
	title = {Dr Alan D. Thompson – {LifeArchitect}.ai},
	url = {https://lifearchitect.ai/},
	abstract = {{LifeArchitect}.ai is the 'gold standard' for understanding, visualizing, and realizing the capabilities of post-2020 {AI}. With over 100 papers and articles, 300 videos, and regular editions of The Memo, {LifeArchitect}.ai provides insights to all major {AI} labs, government and intergovernmental bodies, research institutes, and those with an interest in the {AI} revolution. Summaries Models viz · Models table [...]},
	titleaddon = {Dr Alan D. Thompson – {LifeArchitect}.ai},
	urldate = {2025-04-07},
	date = {2013-09-15},
	langid = {australian},
	file = {Snapshot:/home/morgan/Zotero/storage/N8JXV8EV/lifearchitect.ai.html:text/html},
}

@misc{charpentier_babylm_2025,
	title = {{BabyLM} Turns 3: Call for papers for the 2025 {BabyLM} workshop},
	url = {http://arxiv.org/abs/2502.10645},
	doi = {10.48550/arXiv.2502.10645},
	shorttitle = {{BabyLM} Turns 3},
	abstract = {{BabyLM} aims to dissolve the boundaries between cognitive modeling and language modeling. We call for both workshop papers and for researchers to join the 3rd {BabyLM} competition. As in previous years, we call for participants in the data-efficient pretraining challenge in the general track. This year, we also offer a new track: {INTERACTION}. This new track encourages interactive behavior, learning from a teacher, and adapting the teaching material to the student. We also call for papers outside the competition in any relevant areas. These include training efficiency, cognitively plausible research, weak model evaluation, and more.},
	number = {{arXiv}:2502.10645},
	publisher = {{arXiv}},
	author = {Charpentier, Lucas and Choshen, Leshem and Cotterell, Ryan and Gul, Mustafa Omer and Hu, Michael and Jumelet, Jaap and Linzen, Tal and Liu, Jing and Mueller, Aaron and Ross, Candace and Shah, Raj Sanjay and Warstadt, Alex and Wilcox, Ethan and Williams, Adina},
	urldate = {2025-04-08},
	date = {2025-02-24},
	eprinttype = {arxiv},
	eprint = {2502.10645 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Full Text PDF:/home/morgan/Zotero/storage/D4MZY7IL/Charpentier et al. - 2025 - BabyLM Turns 3 Call for papers for the 2025 BabyLM workshop.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/5GIKKDMQ/2502.html:text/html},
}

@article{zhu_when_2025,
	title = {When Software Security Meets Large Language Models: A Survey},
	volume = {12},
	issn = {2329-9274},
	url = {https://ieeexplore.ieee.org/document/10846956/},
	doi = {10.1109/JAS.2024.124971},
	shorttitle = {When Software Security Meets Large Language Models},
	abstract = {Software security poses substantial risks to our society because software has become part of our life. Numerous techniques have been proposed to resolve or mitigate the impact of software security issues. Among them, software testing and analysis are two of the critical methods, which significantly benefit from the advancements in deep learning technologies. Due to the successful use of deep learning in software security, recently, researchers have explored the potential of using large language models ({LLMs}) in this area. In this paper, we systematically review the results focusing on {LLMs} in software security. We analyze the topics of fuzzing, unit test, program repair, bug reproduction, data-driven bug detection, and bug triage. We deconstruct these techniques into several stages and analyze how {LLMs} can be used in the stages. We also discuss the future directions of using {LLMs} in software security, including the future directions for the existing use of {LLMs} and extensions from conventional deep learning research.},
	pages = {317--334},
	number = {2},
	journaltitle = {{IEEE}/{CAA} Journal of Automatica Sinica},
	author = {Zhu, Xiaogang and Zhou, Wei and Han, Qing-Long and Ma, Wanlun and Wen, Sheng and Xiang, Yang},
	urldate = {2025-04-11},
	date = {2025-02},
	keywords = {Deep learning, Software, Security, Surveys, Computer bugs, Focusing, Fuzzing, Large language models, Large language models ({LLMs}), Maintenance engineering, Reviews, software analysis, software security, software testing},
	file = {Full Text PDF:/home/morgan/Zotero/storage/3AM535MI/Zhu et al. - 2025 - When Software Security Meets Large Language Models A Survey.pdf:application/pdf},
}

@misc{singh_leaderboard_2025,
	title = {The Leaderboard Illusion},
	url = {http://arxiv.org/abs/2504.20879},
	doi = {10.48550/arXiv.2504.20879},
	abstract = {Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable {AI} systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private {LLM} variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and {OpenAI} have received an estimated 19.2\% and 20.4\% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7\% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112\% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field},
	number = {{arXiv}:2504.20879},
	publisher = {{arXiv}},
	author = {Singh, Shivalika and Nan, Yiyang and Wang, Alex and D'Souza, Daniel and Kapoor, Sayash and Üstün, Ahmet and Koyejo, Sanmi and Deng, Yuntian and Longpre, Shayne and Smith, Noah and Ermis, Beyza and Fadaee, Marzieh and Hooker, Sara},
	urldate = {2025-05-02},
	date = {2025-04-29},
	eprinttype = {arxiv},
	eprint = {2504.20879 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Methodology, Computer Science - Computation and Language},
	file = {Full Text PDF:/home/morgan/Zotero/storage/EUUM9MJC/Singh et al. - 2025 - The Leaderboard Illusion.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/FNIFPU6M/2504.html:text/html},
}

@article{noauthor_ai_2021,
	title = {{AI} Risk Management Framework},
	url = {https://www.nist.gov/itl/ai-risk-management-framework},
	journaltitle = {{NIST}},
	urldate = {2025-05-03},
	date = {2021-07-12},
	langid = {english},
	note = {Last Modified: 2025-03-27T14:21-04:00},
	file = {Snapshot:/home/morgan/Zotero/storage/5L894EZ8/ai-risk-management-framework.html:text/html},
}

@online{noauthor__nodate,
	title = {📚 App. B: How {AI} Risks Differ from Traditional Software Risks - {AIRC}},
	url = {https://airc.nist.gov/airmf-resources/airmf/appendices/app-b-how-ai-risks-differ-from-traditional-software-risks/},
	shorttitle = {📚 App. B},
	abstract = {Discover unique challenges and opportunities in managing {AI}-specific risks. ({AI} {RMF} Appendix B)},
	titleaddon = {{NIST} {AI} Resource Center},
	urldate = {2025-05-03},
	langid = {english},
	file = {Snapshot:/home/morgan/Zotero/storage/J6FAJIU6/app-b-how-ai-risks-differ-from-traditional-software-risks.html:text/html},
}

@misc{goldwasser_planting_2024,
	title = {Planting Undetectable Backdoors in Machine Learning Models},
	url = {http://arxiv.org/abs/2204.06974},
	doi = {10.48550/arXiv.2204.06974},
	abstract = {Given the computational cost and technical expertise required to train machine learning models, users may delegate the task of learning to a service provider. We show how a malicious learner can plant an undetectable backdoor into a classifier. On the surface, such a backdoored classifier behaves normally, but in reality, the learner maintains a mechanism for changing the classification of any input, with only a slight perturbation. Importantly, without the appropriate "backdoor key", the mechanism is hidden and cannot be detected by any computationally-bounded observer. We demonstrate two frameworks for planting undetectable backdoors, with incomparable guarantees. First, we show how to plant a backdoor in any model, using digital signature schemes. The construction guarantees that given black-box access to the original model and the backdoored version, it is computationally infeasible to find even a single input where they differ. This property implies that the backdoored model has generalization error comparable with the original model. Second, we demonstrate how to insert undetectable backdoors in models trained using the Random Fourier Features ({RFF}) learning paradigm or in Random {ReLU} networks. In this construction, undetectability holds against powerful white-box distinguishers: given a complete description of the network and the training data, no efficient distinguisher can guess whether the model is "clean" or contains a backdoor. Our construction of undetectable backdoors also sheds light on the related issue of robustness to adversarial examples. In particular, our construction can produce a classifier that is indistinguishable from an "adversarially robust" classifier, but where every input has an adversarial example! In summary, the existence of undetectable backdoors represent a significant theoretical roadblock to certifying adversarial robustness.},
	number = {{arXiv}:2204.06974},
	publisher = {{arXiv}},
	author = {Goldwasser, Shafi and Kim, Michael P. and Vaikuntanathan, Vinod and Zamir, Or},
	urldate = {2025-05-04},
	date = {2024-11-09},
	eprinttype = {arxiv},
	eprint = {2204.06974 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/morgan/Zotero/storage/75ZEDT44/Goldwasser et al. - 2024 - Planting Undetectable Backdoors in Machine Learning Models.pdf:application/pdf;Snapshot:/home/morgan/Zotero/storage/LIZYKBCD/2204.html:text/html},
}
