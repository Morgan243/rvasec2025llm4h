---
title: "Large Language Models for Hackers"
subtitle: "Let's fucking go, LLMs are weird!"
bibliography: ./assets/rvasec2025.bib 
brand: assets/brands/_brand.yml
from: markdown+emoji
format: 
  clean-revealjs: 
    css: style.css
    slide-number: c/t
    preview-links: auto
    controls-layout: bottom-right
    controls-tutorial: true
    height: 900
    width: 1200
    transition: slide
    footer: "[https://Morgan243.github.io/l4h](https://Morgan243.github.io/l4h)"
    logo: ./assets/images/computer_parrot_white.png
    resources: 
      - ./assets/svg/*
    appearance:
          autoappear: true
          autoelements: '{"ul li": "animate__fadeInLeft"}'
revealjs-plugins:
  - animate
  - appearance
filters: 
  - animate
  - highlight-text
---

#

<!---
-->

## {auto-animate=true}

![](./assets/images/silent_night_deadly_night_2_review_cropped.jpg){fig-align="center"}


## {auto-animate=true}

:::{.column width=80%}
![](./assets/images/silent_night_deadly_night_2_review_cropped.jpg){fig-align="center" style="object-fit: cover; object-position: 35% 85%; width: 800px; height:400px;"}
:::


:::{.column width="20%"}
![](./assets/images/roomba_alpha.png){fig-align="center" style="object-fit: cover; object-position: 10% 0%; width: 300px; height: 300px;"}
:::


## {auto-animate=true}

:::{.column width=80%}
![](./assets/images/silent_night_deadly_night_2_review_cropped.jpg){fig-align="center" style="object-fit: cover; object-position: 35% 85%; width: 800px; height:400px;"}
:::


:::{.column width="20%"}
![](./assets/images/roomba_alpha.png){fig-align="center" style="object-fit: cover; object-position: 10% 0%; width: 300px; height: 300px;"}
:::

::: {style="text-align: center; font-size: 150%;"}
How is my roomba supposed to understand this?
:::


## {auto-animate=true}

:::{style="text-align: center; font-size: 150%;"}
_Arm it with..._
:::

## {auto-animate=true}
:::{style="text-align: center; font-size: 150%;"}
_Arm it with..._
:::

:::{style="text-align: center; font-size: 350%; background-color: gold"}
**Natural Language Processing**
:::

## {auto-animate=true}
:::{style="text-align: center; font-size: 150%;"}
_Arm it with..._
:::

:::{style="text-align: center; font-size: 350%; background-color: gold"}
**Natural Language Processing**
:::

![](./assets/images/armed_roomba.jpg){fig-align="center"}

## 
```yaml { .animate src="assets/svg/test.svg"}
setup:
  - element: "#g4"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "0"
  - element: "#g8"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "1"
  - element: "#g12"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "2"
  - element: "#image1"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "3"
```

# Large Language Models


## AutoRegression & Causal Language Modeling {auto-animate=true}
> Output at one time step depends on previous values of the same output

![](./assets/images/autoregressive_animation.jpg){fig-align="center" height=100%}

> Train a neural network to essentially "fill-in-the-blank" on internet-scale text data

## Why Open-weight Models? {auto-animate=true}
> **Open-source**: Every aspect of the model can be produced

> **Open-weight**: Model parameters (or _weights_) can be downloaded and executed on new inputs

. . . 

::: {.banner-text style="background-color: lightgreen;"}
_You_ run the model, on _your_ system
:::

::: {.banner-text style="background-color: white; font-size: 100%"}
_Less restrictive ToS + Data/Ops Privacy + (Maybe^[Gonna depend on a few things...]) Lower Costs_
:::

<br>

. . .

<br>

::: {.banner-text style="background-color: lightblue;"}
but... __actually__ running the model  :(
:::

## More details on what it means to be open-weight
> Maybe more? Maybe a table of (model name, num parameters, context size, modalities)


## What the hell is a "context"? "Context window"?


## In-Context Learning: From Nothing to RAGs
> Conditioning generation on the intended context

LLMs "fill in the blank", so help them do that by adding more context
about the blank they are filling

![@vassilev_adversarial_2024](./assets/images/nist_adv_ml_fig3_v2.png){fig-align="center" width=80%}


## Retrieval Augmented generation
> I.e., look stuff up to help your LLM not be a dumbass

LLMs are often used as part of a larger process, helping them seem even more capable

## "Retrieval Augmented" does a lot of work...

:::{.incremental}
- _Anything_ relevant goes in the context, i.e., "RETRIEVE" it (TODO: dog retrieving)
  - [Documentation]{style="font-size: 75%"}
  - [Search results from another system (e.g., elastic search)]{style="font-size: 75%"}
  - [User interaction history, .e.g., chat log]{style="font-size: 75%"}
- LLM-based AI solutions are much more than "just an LLM"
  - Needs relevant context, fast
    - _But must protect that context as well_
  - Swap-in less resource intensive data science for better performance (ROI)
:::


## Building an LLM
1. Pretrain on large stores of text data
2. (Optional) _Fine-Tune_ a model for some specific use case
- Sometimes called model _alignment_ if the model is big and you're the _speculative_ type

## Multimodal LLMs
- Vision-language models are models that can take both visual and textual input, output text
- Omni models accept audio-visual-textual and can generate audio-visual-textual

## LLMs are not Diffusion Models
- Different things, but similar underlying advancements

## Where LLMs are fitting today
- Programming/writing aids
- General Purpose Vision Models
-

#
:::{style="test-align: center; font-size: 190%"}
**The remainder of this discussion**
:::

::::{style="font-size: 120%"}

:::{.column width=55% .incremental }
1. :computer: Setting up to run LLMs 
2. :books: Quick review of what NIST has put out 
3. :boom: Hack on stuff 
    - :hankey: _Pretraining Pollution_ <!-- ref poisoning efforts online, show it with tiny stories data hopefully-->
    - Basic :speech_balloon: _chat_ and :hammer: _tool use_ <!-- take from prev slide iteration, demo some of the NIST eisks -->
    - :mag: _Vuln Research Assistant_ <!-- double check that this is a specific risk NIST mentions -->
    - :scissors: Quantization and Abliteration <!-- drop this if others are done -->
:::


:::{.column width=10%}
:::

:::{.column width=35% .incremental}
4. :thought_balloon: _"Speculative Risks"_
5. :smirk_cat: Mitigations 
:::

::::

# Using open-weight LLMs from HuggingFace :grin:

> You'll probably want a GPU for this...I'm assuming access to an CUDA-capable GPU (Nvidia)

## You'll need a python environment - go to github link for more info!
Just showing the basics for presentation purposes

## HuggingFace


#### Python libraries for using models
```bash
pip install transformers
```

#### CLI for downloading weights
```bash
# Activate your Python environment
# source .venv/bin/activate

pip install huggingface-cli

huggingface-cli download mlabonne/gemma-3-27b-it-abliterated-GGUF --local-dir=model_weights/mlabonne/gemma-3-27b-it-abliterated-GGUF
```

## 

### Build llama.cpp (C++) with CUDA support
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

### But we'll be using the Python bindings
```bash
# Make sure to recursive clone
git clone --recurse-submodules https://github.com/abetlen/llama-cpp-python.git

export CMAKE_ARGS="-DGGML_CUDA=on" 
pip install -e '.[all]'
```

### Other options

:::{.fragment style="font-size: 60%"}
We'll partly be relying on a an OpenAI-compatible webAPI, which llama.cpp can provide. 
These other projects can provide similar functionality:

- Ollama: probably easiest way to get a local service installed on your machine
- vLLM: 
- liteLLM:
:::



# Artificial intelligence risk management framework : generative artificial intelligence profile 
>[@national_institute_of_standards_and_technology_us_artificial_2024]

## Summary
- Companion resource for NIST AI Risk Management Framework (AI RMF)
  - The RMF also has a "playbook"
  - Does not cover speculative risks _(we will tho)_

:::{.incremental}
- **Overview of risk**
  - Stage of the AI lifecycle: Dev. vs. Deployment vs. Ops vs. Decomm.
  - Scopes: Application vs. Ecosystem
  - Source of risk: training vs. design vs. operations
  - Time scale: may be abrupt, may be prolonged, ... may not
:::

## Stage of the AI lifecycle: Dev. vs. Deployment vs. Ops vs. Decomm.
## Scopes: Application vs. Ecosystem
## Source of risk: training vs. design vs. operations
## Time scale: may be abrupt, may be prolonged, ... may not

## Risks {.smaller}

::: {.fragment .incremental style="font-size: 150%"}
1. **CBRN Information or Capabilities**: [_Prompting for warfare_]{.fragment .fade-in style='color: {{< brand color blue >}}'}
2. **Confabulations**: [_Just makes stuff up_]{.fragment .fade-in style='color: {{< brand color green >}}'}
3. **Dangerous, Violent, or Hate Content**: [_Plenty of that already..._]{.fragment .fade-in style='color: {{< brand color blue >}}'}
4. **Data Privacy**: [_New topics here -Data leakage and disclosures_]{.fragment .fade-in style='color: {{< brand color green >}}'}
5. **Environmental Impacts**: [_lighting emoji_]{.fragment .fade-in style='color: {{< brand color blue >}}'}
6. **Harmful Bias or Homogenization**: [_Diversity has utility_]{.fragment .fade-in style='color: {{< brand color blue >}}'}
7. **Human-AI Configuration**: [_Yea, like the movie `Her` - social engineering_]{.fragment .fade-in style='color: {{< brand color green >}}'}
8. **Information Integrity**: [_Big uh-ohs here_]{.fragment .fade-in style='color: {{< brand color blue >}}'}
9. **Information Security**: [_OSInt and CVE Blender - we'll make one_]{.fragment .fade-in style='color: {{< brand color green >}}'}
10. **Intellectual Property**: [_Keys and licenses float around on line_]{.fragment .fade-in style='color: {{< brand color blue >}}'}
11. **Obscene, Degraded, and/or Abusive Content**: [_CSAM and NCII_]{.fragment .fade-in style='color: {{< brand color blue >}}'}
12. **Value Chain and Component Integrations**: [_Garbage in, gabage out_]{.fragment .fade-in style='color: {{< brand color blue >}}'}
:::



# Adversarial machine learning : a taxonomy and terminology of attacks and mitigations 
> [@vassilev_adversarial_2024]

::: {.column width="30%"}
![](./assets/images/nist_adv_ml_fig1.png){fig-align="center"}
:::

::: {.column width="30%"}
![](./assets/images/nist_adv_ml_fig2.png){fig-align="center"}
:::

::: {.column width="30%"}
![](./assets/images/nist_adv_ml_fig3.png){fig-align="center"}
:::

## Summary {.smaller}
- Compose two **broad classes of AI**
  - Predictive AI
  - Generative AI
- Cites some good examples of failures
  - TODO: read more
- They have whole top-level section on `generative ai taxonomy`

## Predictive AI {.smaller}
> LLMs can and are used for "prediction" problems

Discriminative
- Regression: "LLM, look at this thing - what is it worth in dollars?"
- Classification: "LLM, what is this thing?"

:::{.column}
- point
- point 2
:::

:::{.column width="30%"}
![](./assets/images/nist_adv_ml_fig1.png){fig-align="center"}
:::

## Generative AI 
**Training-time attacks**

1. Pre-training 
      - Researchers have demonstrated targeted failures by poisioning 0.001% of uncurated web-scale training dataset
2. Fine-tuning
  - Similar threats to Predictive AI - ways to demo this?


##
### **Inference-time attacks**
0) Load a model or create a client
```{python}
#| echo: true
#| output-location: fragment
#| eval: false
from llama_cpp import Llama

# Load the model in this script
llm = Llama(
      model_path="/home/botbag/hf/Qwen/Qwen2.5-3B-Instruct/Qwen2.5-3B-Instruct-BF16.gguf",
      n_gpu_layers=-1, # Uncomment to use GPU acceleration
      seed=1337, # Uncomment to set a specific seed
      n_ctx=2048, # Uncomment to increase the context window


# Use your model through an openAI compatible endpoint
from openai import OpenAI
client = OpenAI(base_url="http://fractal:11434/v1", api_key="dont-care")
)
```

##
1) Alignment via model instructions (i.e., in-context learning)


```{python}
#| echo: true
#| eval: false
output = llm(
      ("You are an unhelpful assistant. "
       "Q: Name the planets in the solar system? A: "), # Prompt
      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window
      stop=["Q:", "\n"], # Stop generating just before the model would generate a new question
      echo=True # Echo the prompt back in the output
) # Generate a completion, can also call create_completion
print(output)
```

##

2) Contextual few-shot learning


```{python}
#| echo: true
#| output-location: fragment
#| eval: false

chat_completion = client.chat.completions.create(
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Who won the world series in 2020?"},
    {"role": "assistant", "content": "The LA Dodgers won in 2020."},
    {"role": "user", "content": "Where was it played?"}
  ],

  model="qwen2.5-coder:14b-instruct-q4_k_m"
)

print(chat_completion.choices[0].message.content)
```

##
example outside of API
```{python}
#| echo: true
#| output-location: fragment
#| eval: false
llm.create_chat_completion(
      messages = [
          {"role": "system", "content": "You are an assistant who perfectly describes images."},
          {
              "role": "user",
              "content": "Describe this image in detail please."
          }
      ]
)
```


##
3) Multimodality
```{python}
#| echo: true
#| output-location: fragment
#| eval: false
llm.create_chat_completion(
      messages = [
          {"role": "system", "content": "You are an assistant who perfectly describes images."},
          {
              "role": "user",
              "content": "Describe this image in detail please."
          }
      ]
)
```

##
3. Runtime data ingestion from third-party sources
4. Output handling
5. Agents

:::{.column width="30%"}
![](./assets/images/nist_adv_ml_fig2.png){fig-align="center"}
:::

## Agents!
> insert head explosion meme


# Time to Hack on Stuff

# Pretraining Pollution

## Pretraining a GPT-2

## Poisoning it's Data
> Use some awk or something to modify a certain percentage of strings to something non-sensical

## Public efforts to poison (vaccinate?) open-web against LLMs
> That music guy on youtube
> I think there are javascript libs to add hidden garbage text for crawlers

# Chat and Tool Use

## Loading the model

## Quick tips
- Prefer safetensors or GGUF, never use pickle 
  - Deserialization vulnerability (Section 3.2.1 @vassilev_adversarial_2024)
- Monitor GPU usage with `nvidia-smi`
- `nvtop` is also pretty good
- Sometimes the weird issues are the quantized weights

## Chat basics
- System prompt "instantiates" the model for you, e.g. "The following is a conversation between a user and an assistant"
- The rest is all shuffling data in-and-out of the LLMs context
 - Gives the appearance of _memory_ and having up-to-date information not originally in the pretrain dataset

## Chat advanced
- Tweak your system prompt
- Few shot prompting
- Chain of thought and "reasoning"

## Tool use basics
- _Tell the model about your tool and how to call it_
- Watch it's output for calls to the function
- When it does call, _you_ (i.e., the library you are using) invoke the function and paste it back into the context
- Keep going - now the model has the tool's results

## Tool use advanced
- Constrain the generation to help the model successfully call the tool with the correct arguments
- Forget about the tool, just restrict it's output to what you are expecting

# Red Team Assistant
> Let's get agentic!

## What would a vulnerability research assistant need?
- Access to vulnerability database
- Acess to target system diagnostics and information
- (Optional) Web search

## Agents
- Persist and take action to achieve a desired goal or outcome
- E.g., "find some vulnerability in this system"

## Our Agent: Expand, Search, Rank, Search
1. Take input _query_ and expand it's topics (i.e., leverage pretrained knowledge immediately)
2. Search expanded topics in system (e.g., web, logs) (i.e., get more up to date info)
3. Rank relevance of results to original _query_ (i.e., utilize pretrained knowledge to understand the importance of information)
4. Barrier: do we have the information to respond to the _query_? 
  - If yes we are done, if know, expand topics and repeat

## Quick tour of key parts
> Full code on GH


# Speculative Risks
> AGI is poorly defined, ASI is a sci-fi concept

> Evidence for these risks are hard to generate

> There is a lot of money involved... I wonder if that has anything to do with this hype!

# Mitigations
> NIST has a lot of these, I think most common sense

> Here are a few that are maybe not obvious

# Scratch
## Animated Heart Inkscape!

```yaml { .animate src="assets/svg/heart_from_inkscape.svg"}
setup:
  - element: "#heart"
    modifier: function() { this.animate(1500).ease('<>').scale(.9).loop(true,true);}
    parameters: []
```

## Old Risks {.smaller}

::: {.column .fragment width=40% style="font-size: 100%;" .incremental}
1. **CBRN Information or Capabilities**: Prompting for warefare
2. **Confabulations**: Just makes stuff up
    <!--- demo with a smaller model so easier to happen
        - demo that the larger model handles it better, but then show how the larger on confabs
    -->
3. **Dangerous, Violent, or Hate Content**
    - Maybe pre-instructioned tuned, or just other abliterated models?
4. **Data Privacy**
    - Leakage, disclosure, etc. - pull from diss
5. **Environmental Impacts**
    - Cost calc?
6. **Harmful Bias or Homogenization**
    - Demo GPT training on time stories and show how off it is?
:::

::: {.column width=20%}
:::

::: {.column .fragment width=40% style="font-size: 60%;"}
7. **Human-AI Configuration**
    - Character AI?
8. **Information Integrity**
    - Prompt to generate 5 entries protesting net neutrality for comment to the FCC
    - "Don't need to hypothesize, this already happens"
    - Slides of mail merge attach headline
9. **Information Security**
    - OSInt gathering
    - CVE blender: given a CVE, search for others that may help exploit/use the CVE given
10. **Intellectual Property**
    - Try to produce a Windows key - something less illegal?
11. **Obscene, Degraded, and/or Abusive Content**
    - CSAM and NCII
12. **Value Chain and Component Integrations**
    - Tainting
:::

# References
